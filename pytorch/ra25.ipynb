{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning\n",
    "\n",
    "\n",
    "class RandomAssociator(pytorch_lightning.LightningModule):\n",
    "    def __init__(self, io_size: int, hidden_size: int, data_loader: torch.utils.data.DataLoader, learning_rate:float=1e-3):\n",
    "        super().__init__()\n",
    "        # the name of this attribute is important to work with\n",
    "        # pytorch_lightning.Trianer(auto_lr_find=True)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.data_loader = data_loader\n",
    "        # TODO: datatypes don't make sense here.\n",
    "        # need to figure out how to represent a bit vector in input and output\n",
    "        # properly.\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(io_size, hidden_size, dtype=torch.uint8),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size, io_size, dtype=torch.uint8),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def training_step(self, batch, unused_batch_idx):\n",
    "        x, y = batch\n",
    "        # enforce batch size == 1 for equivalence to Leabra model\n",
    "        if x.size()[0] != 1 or y.size()[0] != 1:\n",
    "            raise ValueError(\"expected batch size == 1, got\", x.size()[0])\n",
    "        x, y = torch.squeeze(x, 0), torch.squeeze(y, 0)\n",
    "        return F.mse_loss(self(x), y)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class RandomAssociationDataset(torch.utils.data.Dataset):\n",
    "    @staticmethod\n",
    "    def random_datum(\n",
    "        rng: np.random.Generator, num_nonzero: int, size: int\n",
    "    ) -> torch.Tensor:\n",
    "        nonzero_idx = rng.choice(size, num_nonzero)\n",
    "        ret = torch.zeros(size, dtype=torch.uint8)\n",
    "        ret[nonzero_idx] = 1\n",
    "        return ret\n",
    "\n",
    "    def __init__(self, num_nonzero: int, datum_size: int, size: int):\n",
    "        super().__init__()\n",
    "        rng = np.random.default_rng()\n",
    "        self.xs = [\n",
    "            self.random_datum(rng, num_nonzero, datum_size) for _ in range(size)\n",
    "        ]\n",
    "        self.ys = [\n",
    "            self.random_datum(rng, num_nonzero, datum_size) for _ in range(size)\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.xs[idx], self.ys[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Byte but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/guest_person/src/ra25-torch/notebook.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/guest_person/src/ra25-torch/notebook.ipynb#ch0000002vscode-remote?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m RandomAssociator(datum_size, hidden_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, data_loader\u001b[39m=\u001b[39mdata_loader)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/guest_person/src/ra25-torch/notebook.ipynb#ch0000002vscode-remote?line=6'>7</a>\u001b[0m \u001b[39m# find the learning rate\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/guest_person/src/ra25-torch/notebook.ipynb#ch0000002vscode-remote?line=7'>8</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtune(model)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/guest_person/src/ra25-torch/notebook.ipynb#ch0000002vscode-remote?line=9'>10</a>\u001b[0m trainer\u001b[39m.\u001b[39mfit(model)\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1099\u001b[0m, in \u001b[0;36mTrainer.tune\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, scale_batch_size_kwargs, lr_find_kwargs, train_dataloader)\u001b[0m\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1093'>1094</a>\u001b[0m \u001b[39m# links data to the trainer\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1094'>1095</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1095'>1096</a>\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1096'>1097</a>\u001b[0m )\n\u001b[0;32m-> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1098'>1099</a>\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtuner\u001b[39m.\u001b[39;49m_tune(model, scale_batch_size_kwargs\u001b[39m=\u001b[39;49mscale_batch_size_kwargs, lr_find_kwargs\u001b[39m=\u001b[39;49mlr_find_kwargs)\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1100'>1101</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1101'>1102</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtuning \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/tuner/tuning.py:53\u001b[0m, in \u001b[0;36mTuner._tune\u001b[0;34m(self, model, scale_batch_size_kwargs, lr_find_kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/tuner/tuning.py?line=50'>51</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mauto_lr_find:\n\u001b[1;32m     <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/tuner/tuning.py?line=51'>52</a>\u001b[0m     lr_find_kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mupdate_attr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/tuner/tuning.py?line=52'>53</a>\u001b[0m     result[\u001b[39m\"\u001b[39m\u001b[39mlr_find\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m lr_find(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer, model, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mlr_find_kwargs)\n\u001b[1;32m     <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/tuner/tuning.py?line=54'>55</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mFINISHED\n\u001b[1;32m     <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/tuner/tuning.py?line=56'>57</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py:238\u001b[0m, in \u001b[0;36mlr_find\u001b[0;34m(trainer, model, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr)\u001b[0m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py?line=234'>235</a>\u001b[0m trainer\u001b[39m.\u001b[39minit_optimizers \u001b[39m=\u001b[39m lr_finder\u001b[39m.\u001b[39m_exchange_scheduler(trainer)\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py?line=236'>237</a>\u001b[0m \u001b[39m# Fit, lr & loss logged in callback\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py?line=237'>238</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtuner\u001b[39m.\u001b[39;49m_run(model)\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py?line=239'>240</a>\u001b[0m \u001b[39m# Prompt if we stopped early\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py?line=240'>241</a>\u001b[0m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mglobal_step \u001b[39m!=\u001b[39m num_training:\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/tuner/tuning.py:63\u001b[0m, in \u001b[0;36mTuner._run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/tuner/tuning.py?line=60'>61</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING  \u001b[39m# last `_run` call might have set it to `FINISHED`\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/tuner/tuning.py?line=61'>62</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/tuner/tuning.py?line=62'>63</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_run(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/tuner/tuning.py?line=63'>64</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtuning \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1199\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1195'>1196</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheckpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1197'>1198</a>\u001b[0m \u001b[39m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1198'>1199</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch()\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1200'>1201</a>\u001b[0m \u001b[39m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1201'>1202</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_dispatch()\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1279\u001b[0m, in \u001b[0;36mTrainer._dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1276'>1277</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_type_plugin\u001b[39m.\u001b[39mstart_predicting(\u001b[39mself\u001b[39m)\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1277'>1278</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1278'>1279</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_type_plugin\u001b[39m.\u001b[39;49mstart_training(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:202\u001b[0m, in \u001b[0;36mTrainingTypePlugin.start_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py?line=199'>200</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstart_training\u001b[39m(\u001b[39mself\u001b[39m, trainer: \u001b[39m\"\u001b[39m\u001b[39mpl.Trainer\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py?line=200'>201</a>\u001b[0m     \u001b[39m# double dispatch to initiate the training loop\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py?line=201'>202</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mrun_stage()\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1289\u001b[0m, in \u001b[0;36mTrainer.run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1286'>1287</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1287'>1288</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1288'>1289</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1319\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1316'>1317</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1317'>1318</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1318'>1319</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=142'>143</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=143'>144</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=144'>145</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=145'>146</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=146'>147</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:234\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py?line=230'>231</a>\u001b[0m data_fetcher \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mget_profiled_dataloader(dataloader)\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py?line=232'>233</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py?line=233'>234</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(data_fetcher)\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py?line=235'>236</a>\u001b[0m     \u001b[39m# the global step is manually decreased here due to backwards compatibility with existing loggers\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py?line=236'>237</a>\u001b[0m     \u001b[39m# as they expect that the same step is used when logging epoch end metrics even when the batch loop has\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py?line=237'>238</a>\u001b[0m     \u001b[39m# finished. this means the attribute does not exactly track the number of optimizer steps applied.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py?line=238'>239</a>\u001b[0m     \u001b[39m# TODO(@carmocca): deprecate and rename so users don't get confused\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py?line=239'>240</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=142'>143</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=143'>144</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=144'>145</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=145'>146</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=146'>147</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:193\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=189'>190</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=191'>192</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=192'>193</a>\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_loop\u001b[39m.\u001b[39;49mrun(batch, batch_idx)\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=194'>195</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=196'>197</a>\u001b[0m \u001b[39m# update non-plateau LR schedulers\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=197'>198</a>\u001b[0m \u001b[39m# update epoch-interval ones only when we are at the end of training epoch\u001b[39;00m\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=142'>143</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=143'>144</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=144'>145</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=145'>146</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=146'>147</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py:88\u001b[0m, in \u001b[0;36mTrainingBatchLoop.advance\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py?line=85'>86</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mautomatic_optimization:\n\u001b[1;32m     <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py?line=86'>87</a>\u001b[0m     optimizers \u001b[39m=\u001b[39m _get_active_optimizers(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizer_frequencies, batch_idx)\n\u001b[0;32m---> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py?line=87'>88</a>\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer_loop\u001b[39m.\u001b[39;49mrun(split_batch, optimizers, batch_idx)\n\u001b[1;32m     <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py?line=88'>89</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py?line=89'>90</a>\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_loop\u001b[39m.\u001b[39mrun(split_batch, batch_idx)\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=142'>143</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=143'>144</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=144'>145</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=145'>146</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=146'>147</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:215\u001b[0m, in \u001b[0;36mOptimizerLoop.advance\u001b[0;34m(self, batch, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=213'>214</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madvance\u001b[39m(\u001b[39mself\u001b[39m, batch: Any, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=214'>215</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_optimization(\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=215'>216</a>\u001b[0m         batch,\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=216'>217</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_idx,\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=217'>218</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizers[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptim_progress\u001b[39m.\u001b[39;49moptimizer_position],\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=218'>219</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer_idx,\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=219'>220</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=220'>221</a>\u001b[0m     \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=221'>222</a>\u001b[0m         \u001b[39m# automatic optimization assumes a loss needs to be returned for extras to be considered as the batch\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=222'>223</a>\u001b[0m         \u001b[39m# would be skipped otherwise\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=223'>224</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_idx] \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39masdict()\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:266\u001b[0m, in \u001b[0;36mOptimizerLoop._run_optimization\u001b[0;34m(self, split_batch, batch_idx, optimizer, opt_idx)\u001b[0m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=258'>259</a>\u001b[0m         closure()\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=260'>261</a>\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=261'>262</a>\u001b[0m \u001b[39m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=262'>263</a>\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=263'>264</a>\u001b[0m \u001b[39m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=264'>265</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=265'>266</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_step(optimizer, opt_idx, batch_idx, closure)\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=267'>268</a>\u001b[0m result \u001b[39m=\u001b[39m closure\u001b[39m.\u001b[39mconsume_result()\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=269'>270</a>\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=270'>271</a>\u001b[0m     \u001b[39m# if no result, user decided to skip optimization\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=271'>272</a>\u001b[0m     \u001b[39m# otherwise update running loss + reset accumulated loss\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=272'>273</a>\u001b[0m     \u001b[39m# TODO: find proper way to handle updating running loss\u001b[39;00m\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:378\u001b[0m, in \u001b[0;36mOptimizerLoop._optimizer_step\u001b[0;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=374'>375</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_ready()\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=376'>377</a>\u001b[0m \u001b[39m# model hook\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=377'>378</a>\u001b[0m lightning_module\u001b[39m.\u001b[39;49moptimizer_step(\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=378'>379</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mcurrent_epoch,\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=379'>380</a>\u001b[0m     batch_idx,\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=380'>381</a>\u001b[0m     optimizer,\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=381'>382</a>\u001b[0m     opt_idx,\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=382'>383</a>\u001b[0m     train_step_and_backward_closure,\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=383'>384</a>\u001b[0m     on_tpu\u001b[39m=\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_device_type \u001b[39m==\u001b[39;49m DeviceType\u001b[39m.\u001b[39;49mTPU \u001b[39mand\u001b[39;49;00m _TPU_AVAILABLE),\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=384'>385</a>\u001b[0m     using_native_amp\u001b[39m=\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mamp_backend \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39mand\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mamp_backend \u001b[39m==\u001b[39;49m AMPType\u001b[39m.\u001b[39;49mNATIVE),\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=385'>386</a>\u001b[0m     using_lbfgs\u001b[39m=\u001b[39;49mis_lbfgs,\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=386'>387</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=388'>389</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:1652\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=1570'>1571</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimizer_step\u001b[39m(\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=1571'>1572</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=1572'>1573</a>\u001b[0m     epoch: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=1579'>1580</a>\u001b[0m     using_lbfgs: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=1580'>1581</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=1581'>1582</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=1582'>1583</a>\u001b[0m \u001b[39m    Override this method to adjust the default way the\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=1583'>1584</a>\u001b[0m \u001b[39m    :class:`~pytorch_lightning.trainer.trainer.Trainer` calls each optimizer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=1649'>1650</a>\u001b[0m \n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=1650'>1651</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=1651'>1652</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49moptimizer_closure)\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py:164\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py?line=161'>162</a>\u001b[0m \u001b[39massert\u001b[39;00m trainer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py?line=162'>163</a>\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(profiler_action):\n\u001b[0;32m--> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py?line=163'>164</a>\u001b[0m     trainer\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49moptimizer_step(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_idx, closure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py:339\u001b[0m, in \u001b[0;36mAccelerator.optimizer_step\u001b[0;34m(self, optimizer, opt_idx, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py?line=328'>329</a>\u001b[0m \u001b[39m\"\"\"performs the actual optimizer step.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py?line=329'>330</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py?line=330'>331</a>\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py?line=335'>336</a>\u001b[0m \u001b[39m    **kwargs: Any extra arguments to ``optimizer.step``\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py?line=336'>337</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py?line=337'>338</a>\u001b[0m model \u001b[39m=\u001b[39m model \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module\n\u001b[0;32m--> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py?line=338'>339</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecision_plugin\u001b[39m.\u001b[39;49moptimizer_step(model, optimizer, opt_idx, closure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:163\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[0;34m(self, model, optimizer, optimizer_idx, closure, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=160'>161</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, pl\u001b[39m.\u001b[39mLightningModule):\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=161'>162</a>\u001b[0m     closure \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_closure, model, optimizer, optimizer_idx, closure)\n\u001b[0;32m--> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=162'>163</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/optim/lr_scheduler.py?line=62'>63</a>\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/optim/lr_scheduler.py?line=63'>64</a>\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/optim/lr_scheduler.py?line=64'>65</a>\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/optim/optimizer.py?line=85'>86</a>\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/optim/optimizer.py?line=86'>87</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/optim/optimizer.py?line=87'>88</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=23'>24</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/optim/adam.py:100\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/optim/adam.py?line=97'>98</a>\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/optim/adam.py?line=98'>99</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[0;32m--> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/optim/adam.py?line=99'>100</a>\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/optim/adam.py?line=101'>102</a>\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/optim/adam.py?line=102'>103</a>\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:148\u001b[0m, in \u001b[0;36mPrecisionPlugin._wrap_closure\u001b[0;34m(self, model, optimizer, optimizer_idx, closure)\u001b[0m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=134'>135</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap_closure\u001b[39m(\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=135'>136</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=136'>137</a>\u001b[0m     model: \u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=139'>140</a>\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=140'>141</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=141'>142</a>\u001b[0m     \u001b[39m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=142'>143</a>\u001b[0m \u001b[39m    ``on_before_optimizer_step`` hook is called.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=143'>144</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=144'>145</a>\u001b[0m \u001b[39m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=145'>146</a>\u001b[0m \u001b[39m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=146'>147</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=147'>148</a>\u001b[0m     closure_result \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=148'>149</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_closure(model, optimizer, optimizer_idx)\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=149'>150</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:160\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=158'>159</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=159'>160</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclosure(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=160'>161</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39mloss\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:142\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=139'>140</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclosure\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ClosureResult:\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=140'>141</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_profiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mtraining_step_and_backward\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=141'>142</a>\u001b[0m         step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step_fn()\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=143'>144</a>\u001b[0m         \u001b[39mif\u001b[39;00m step_output\u001b[39m.\u001b[39mclosure_loss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=144'>145</a>\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarning_cache\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=145'>146</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=146'>147</a>\u001b[0m             )\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:435\u001b[0m, in \u001b[0;36mOptimizerLoop._training_step\u001b[0;34m(self, split_batch, batch_idx, opt_idx)\u001b[0m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=432'>433</a>\u001b[0m lightning_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtraining_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=433'>434</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mtraining_step\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=434'>435</a>\u001b[0m     training_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mtraining_step(step_kwargs)\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=435'>436</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtraining_type_plugin\u001b[39m.\u001b[39mpost_training_step()\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=437'>438</a>\u001b[0m \u001b[39mdel\u001b[39;00m step_kwargs\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py:219\u001b[0m, in \u001b[0;36mAccelerator.training_step\u001b[0;34m(self, step_kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py?line=213'>214</a>\u001b[0m \u001b[39m\"\"\"The actual training step.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py?line=214'>215</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py?line=215'>216</a>\u001b[0m \u001b[39mSee :meth:`~pytorch_lightning.core.lightning.LightningModule.training_step` for more details\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py?line=216'>217</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py?line=217'>218</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mtrain_step_context():\n\u001b[0;32m--> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py?line=218'>219</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_type_plugin\u001b[39m.\u001b[39;49mtraining_step(\u001b[39m*\u001b[39;49mstep_kwargs\u001b[39m.\u001b[39;49mvalues())\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:213\u001b[0m, in \u001b[0;36mTrainingTypePlugin.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py?line=211'>212</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraining_step\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py?line=212'>213</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtraining_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/home/guest_person/src/ra25-torch/notebook.ipynb Cell 1'\u001b[0m in \u001b[0;36mRandomAssociator.training_step\u001b[0;34m(self, batch, unused_batch_idx)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/guest_person/src/ra25-torch/notebook.ipynb#ch0000000vscode-remote?line=25'>26</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mexpected batch size == 1, got\u001b[39m\u001b[39m\"\u001b[39m, x\u001b[39m.\u001b[39msize()[\u001b[39m0\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/guest_person/src/ra25-torch/notebook.ipynb#ch0000000vscode-remote?line=26'>27</a>\u001b[0m x, y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqueeze(x, \u001b[39m0\u001b[39m), torch\u001b[39m.\u001b[39msqueeze(y, \u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/guest_person/src/ra25-torch/notebook.ipynb#ch0000000vscode-remote?line=27'>28</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mmse_loss(\u001b[39mself\u001b[39;49m(x), y)\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/guest_person/src/ra25-torch/notebook.ipynb Cell 1'\u001b[0m in \u001b[0;36mRandomAssociator.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/guest_person/src/ra25-torch/notebook.ipynb#ch0000000vscode-remote?line=18'>19</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/guest_person/src/ra25-torch/notebook.ipynb#ch0000000vscode-remote?line=19'>20</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers(x)\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/nn/modules/container.py?line=140'>141</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/nn/modules/container.py?line=141'>142</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/nn/modules/linear.py?line=101'>102</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///home/guest_person/src/ra25-torch/.direnv/python-3.8.10/lib/python3.8/site-packages/torch/nn/modules/linear.py?line=102'>103</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Byte but found Float"
     ]
    }
   ],
   "source": [
    "datum_size = 25\n",
    "dataset = RandomAssociationDataset(num_nonzero=6, datum_size=datum_size, size=100)\n",
    "\n",
    "trainer = pytorch_lightning.Trainer(auto_lr_find=True)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=1)\n",
    "model = RandomAssociator(datum_size, hidden_size=64, data_loader=data_loader)\n",
    "# find the learning rate\n",
    "trainer.tune(model)\n",
    "\n",
    "trainer.fit(model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b742a93cc6678651600365abe4f2e7877a0c5b4293f7da7170366eff14830d83"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('python-3.8.10': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "import pytorch_lightning\n",
    "\n",
    "# reduce log noise\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)\n",
    "\n",
    "class RandomAssociator(pytorch_lightning.LightningModule):\n",
    "    def __init__(self, io_size: int, hidden_size: int,\n",
    "                 data_loader: torch.utils.data.DataLoader,\n",
    "                 learning_rate:float=1e-3):\n",
    "        super().__init__()\n",
    "        # the name of this attribute is important to work with\n",
    "        # pytorch_lightning.Trainer(auto_lr_find=True)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.data_loader = data_loader\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(io_size, hidden_size, dtype=torch.double),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size, io_size, dtype=torch.double),\n",
    "        )\n",
    "        self.n_zero = 0\n",
    "        self.first_zero = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Conceptually we want the network to output only 0s or 1s.\n",
    "        # So something like:\n",
    "        # torch.ceil(torch.clamp(self.layers(x), min=0, max=1))\n",
    "        # But this results in a failure to make progress during training.\n",
    "        # Not sure why. For now handle it in training_epoch_end.\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        # enforce batch size == 1 for equivalence to Leabra model\n",
    "        if x.size()[0] != 1 or y.size()[0] != 1:\n",
    "            raise ValueError(\"expected batch size == 1, got\", x.size()[0])\n",
    "        x, y = torch.squeeze(x, 0), torch.squeeze(y, 0)\n",
    "        preds = self(x)\n",
    "        loss = torch.nn.functional.l1_loss(preds, y)\n",
    "        self.log(\"loss\", loss, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, outputs) -> None:\n",
    "        max_loss = max(output[\"loss\"] for output in outputs)\n",
    "        # Because the output is not forced to be 0 or 1, the\n",
    "        # loss will never be zero, so we have a threshold.\n",
    "        # This is arbitrary. TODO: pick something to make it a fair\n",
    "        # comparison with Leabra.\n",
    "        if max_loss < 0.05:\n",
    "          self.n_zero += 1\n",
    "          if self.first_zero is None:\n",
    "            self.first_zero = self.current_epoch\n",
    "        self.log(\"n_zero\", float(self.n_zero))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class RandomAssociationDataset(torch.utils.data.Dataset):\n",
    "    @staticmethod\n",
    "    def random_datum(\n",
    "        rng: np.random.Generator, num_nonzero: int, size: int\n",
    "    ) -> torch.Tensor:\n",
    "        nonzero_idx = rng.choice(size, num_nonzero)\n",
    "        ret = torch.zeros(size, dtype=torch.double)\n",
    "        ret[nonzero_idx] = 1\n",
    "        return ret\n",
    "\n",
    "    def __init__(self, num_nonzero: int, datum_size: int, size: int):\n",
    "        super().__init__()\n",
    "        rng = np.random.default_rng()\n",
    "        self.xs = [\n",
    "            self.random_datum(rng, num_nonzero, datum_size) for _ in range(size)\n",
    "        ]\n",
    "        self.ys = [\n",
    "            self.random_datum(rng, num_nonzero, datum_size) for _ in range(size)\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.xs[idx], self.ys[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Astera-org/models/blob/0de0c8005cdc57c28b2c663c89b3741508d013d2/mechs/ra25/ra25.go#L26\n",
    "datum_size = 6 * 6\n",
    "dataset = RandomAssociationDataset(\n",
    "    # https://github.com/Astera-org/models/blob/0de0c8005cdc57c28b2c663c89b3741508d013d2/mechs/ra25/ra25.go#L27\n",
    "    num_nonzero=6,\n",
    "    datum_size=datum_size,\n",
    "    # https://github.com/Astera-org/models/blob/0de0c8005cdc57c28b2c663c89b3741508d013d2/mechs/ra25/ra25.go#L28\n",
    "    size=30)\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "early_stopping = pytorch_lightning.callbacks.EarlyStopping(\n",
    "    \"n_zero\",\n",
    "    patience=sys.maxsize, # effectively infinite\n",
    "    mode=\"max\",\n",
    "    # https://github.com/Astera-org/models/blob/0de0c8005cdc57c28b2c663c89b3741508d013d2/mechs/ra25/ra25.go#L188\n",
    "    stopping_threshold=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pytorch_lightning.Trainer(\n",
    "    auto_lr_find=True,\n",
    "    # https://github.com/Astera-org/models/blob/0de0c8005cdc57c28b2c663c89b3741508d013d2/mechs/ra25/ra25_test.go#L42\n",
    "    max_epochs=100,\n",
    "    callbacks=[early_stopping])\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=1)\n",
    "model = RandomAssociator(datum_size, hidden_size=64, data_loader=data_loader)\n",
    "\n",
    "trainer.fit(model)\n",
    "\n",
    "print(\"first_zero\", model.first_zero)\n",
    "print(\"last_zero\", early_stopping.stopped_epoch)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b742a93cc6678651600365abe4f2e7877a0c5b4293f7da7170366eff14830d83"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('python-3.8.10': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

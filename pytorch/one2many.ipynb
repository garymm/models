{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "import pytorch_lightning\n",
    "import random\n",
    "\n",
    "# reduce log noise\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)\n",
    "\n",
    "class OneToManyAssociator(pytorch_lightning.LightningModule):\n",
    "    def __init__(self, io_size: int, hidden_size: int,\n",
    "                 data_loader: torch.utils.data.DataLoader,\n",
    "                 learning_rate:float=1e-3):\n",
    "        super().__init__()\n",
    "        # the name of this attribute is important to work with\n",
    "        # pytorch_lightning.Trainer(auto_lr_find=True)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.data_loader = data_loader\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(io_size, hidden_size, dtype=torch.double),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size, io_size, dtype=torch.double),\n",
    "        )\n",
    "        self.n_zero = 0\n",
    "        self.first_zero = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Conceptually we want the network to output only 0s or 1s.\n",
    "        # So something like:\n",
    "        # torch.ceil(torch.clamp(self.layers(x), min=0, max=1))\n",
    "        # But this results in a failure to make progress during training.\n",
    "        # Not sure why. For now handle it in training_epoch_end.\n",
    "        return self.layers(x) + torch.tensor(0.5)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, ys = batch\n",
    "        y0, y1 = ys\n",
    "        y = random.choice((y0, y1))\n",
    "        \n",
    "        # enforce batch size == 1 for equivalence to Leabra model\n",
    "        if x.size()[0] != 1 or y.size()[0] != 1:\n",
    "            raise ValueError(\"expected batch size == 1, got\", x.size()[0])\n",
    "        x, y = torch.squeeze(x, 0), torch.squeeze(y, 0)\n",
    "        preds = self(x)\n",
    "        loss = torch.pow(torch.nn.functional.l1_loss(preds, y), 1/4)\n",
    "        self.log(\"loss\", loss, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, outputs) -> None:\n",
    "        max_loss = max(output[\"loss\"] for output in outputs)\n",
    "        # Because the output is not forced to be 0 or 1, the\n",
    "        # loss will never be zero, so we have a threshold.\n",
    "        # This is arbitrary. TODO: pick something to make it a fair\n",
    "        # comparison with Leabra.\n",
    "        if max_loss < 0.05:\n",
    "          self.n_zero += 1\n",
    "          if self.first_zero is None:\n",
    "            self.first_zero = self.current_epoch\n",
    "        self.log(\"n_zero\", float(self.n_zero))\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, ys = batch\n",
    "        y0, y1 = ys\n",
    "        \n",
    "        # enforce batch size == 1 for equivalence to Leabra model\n",
    "        if x.size()[0] != 1 or y0.size()[0] != 1:\n",
    "            raise ValueError(\"expected batch size == 1, got\", x.size()[0])\n",
    "        x, y0, y1 = torch.squeeze(x, 0), torch.squeeze(y0, 0), torch.squeeze(y1, 0)\n",
    "        preds = self(x)\n",
    "        loss0, loss1 = torch.nn.functional.l1_loss(preds, y0), torch.nn.functional.l1_loss(preds, y1)\n",
    "        loss = torch.min(loss0, loss1)\n",
    "        self.log(\"loss\", loss, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.data_loader\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return self.data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class OneToManyDataset(torch.utils.data.Dataset):\n",
    "    @staticmethod\n",
    "    def random_datum(\n",
    "        rng: np.random.Generator, num_nonzero: int, size: int\n",
    "    ) -> torch.Tensor:\n",
    "        nonzero_idx = rng.choice(size, num_nonzero)\n",
    "        ret = torch.zeros(size, dtype=torch.double)\n",
    "        ret[nonzero_idx] = 1\n",
    "        return ret\n",
    "\n",
    "    def __init__(self, num_nonzero: int, datum_size: int, size: int):\n",
    "        super().__init__()\n",
    "        rng = np.random.default_rng()\n",
    "        def rd():\n",
    "            return self.random_datum(rng, num_nonzero, datum_size)\n",
    "        self.xs = [\n",
    "            rd() for _ in range(size)\n",
    "        ]\n",
    "        self.ys = [\n",
    "            (rd(), rd()) for _ in range(size)\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.xs[idx], self.ys[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        dtype=torch.float64),\n",
       " (tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         dtype=torch.float64),\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "          1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "         dtype=torch.float64)))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/Astera-org/models/blob/0de0c8005cdc57c28b2c663c89b3741508d013d2/mechs/ra25/ra25.go#L26\n",
    "datum_size = 6 * 6\n",
    "dataset = OneToManyDataset(\n",
    "    # https://github.com/Astera-org/models/blob/0de0c8005cdc57c28b2c663c89b3741508d013d2/mechs/ra25/ra25.go#L27\n",
    "    num_nonzero=6,\n",
    "    datum_size=datum_size,\n",
    "    # https://github.com/Astera-org/models/blob/0de0c8005cdc57c28b2c663c89b3741508d013d2/mechs/ra25/ra25.go#L28\n",
    "    size=30)\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "early_stopping = pytorch_lightning.callbacks.EarlyStopping(\n",
    "    \"n_zero\",\n",
    "    patience=sys.maxsize, # effectively infinite\n",
    "    mode=\"max\",\n",
    "    # https://github.com/Astera-org/models/blob/0de0c8005cdc57c28b2c663c89b3741508d013d2/mechs/ra25/ra25.go#L188\n",
    "    stopping_threshold=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c965540a924fe391e5fc2fadf2e02b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_zero None\n",
      "last_zero 0\n"
     ]
    }
   ],
   "source": [
    "trainer = pytorch_lightning.Trainer(\n",
    "    #accelerator='gpu', devices=1,\n",
    "    auto_lr_find=True,\n",
    "    # https://github.com/Astera-org/models/blob/0de0c8005cdc57c28b2c663c89b3741508d013d2/mechs/ra25/ra25_test.go#L42\n",
    "    max_epochs=400,\n",
    "    callbacks=[early_stopping])\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=1)\n",
    "model = OneToManyAssociator(datum_size, hidden_size=64, data_loader=data_loader)\n",
    "\n",
    "trainer.fit(model)\n",
    "\n",
    "print(\"first_zero\", model.first_zero)\n",
    "print(\"last_zero\", early_stopping.stopped_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae935f12498a4d999cf5d6dd89edd78f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "          loss             0.056163542822281455\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'loss': 0.056163542822281455}]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.2628e-03, -1.1025e-03,  1.2184e-02, -2.2784e-02,  4.1489e-03,\n",
      "         6.7394e-03,  1.8130e-02, -2.5453e-02,  8.7493e-03, -1.6770e-02,\n",
      "         1.6138e-03, -5.9646e-03, -2.3347e-02,  9.4242e-01,  3.4339e-02,\n",
      "        -1.6437e-02, -7.6369e-03,  9.5718e-01, -6.3848e-03,  1.1124e-02,\n",
      "        -2.4374e-02,  1.4364e-03,  3.1065e-04,  9.6909e-01, -1.0165e-02,\n",
      "         1.4589e-02,  8.6016e-03,  1.6176e-03,  9.3904e-01,  2.6515e-03,\n",
      "         9.7517e-01,  1.7261e-02,  9.4192e-01,  3.7791e-02, -8.7301e-03,\n",
      "        -1.1589e-02], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.])\n",
      "and\n",
      " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64) \n",
      " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "onesample = dataset[10]\n",
    "pred = model(onesample[0])\n",
    "print(pred)\n",
    "print(\"\")\n",
    "print(torch.where(pred > 0.2, 1.0, 0.0))\n",
    "print(\"and\\n\", torch.logical_and(*onesample[1]).to(torch.float))\n",
    "print(onesample[1][0], \"\\n\", onesample[1][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b742a93cc6678651600365abe4f2e7877a0c5b4293f7da7170366eff14830d83"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
